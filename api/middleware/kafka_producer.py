"""
Prime Spark AI - Kafka Event Streaming Middleware
Generated by Autonomous Completion Agent
"""

from fastapi import Request, Response
from starlette.middleware.base import BaseHTTPMiddleware
import asyncio
import time
from datetime import datetime
from typing import Optional
import logging

logger = logging.getLogger(__name__)


class KafkaStreamingMiddleware(BaseHTTPMiddleware):
    """
    Middleware to stream API events to Kafka
    """

    def __init__(self, app, kafka_manager=None):
        super().__init__(app)
        self.kafka_manager = kafka_manager

    async def dispatch(self, request: Request, call_next):
        """Intercept requests and stream events"""
        start_time = time.time()

        # Process request
        response = await call_next(request)

        # Calculate duration
        duration_ms = (time.time() - start_time) * 1000

        # Stream event to Kafka (non-blocking)
        if self.kafka_manager:
            asyncio.create_task(self._stream_event(
                method=request.method,
                path=request.url.path,
                status_code=response.status_code,
                duration_ms=duration_ms,
                user_id=getattr(request.state, 'user_id', None)
            ))

        return response

    async def _stream_event(
        self,
        method: str,
        path: str,
        status_code: int,
        duration_ms: float,
        user_id: Optional[str]
    ):
        """Stream API event to Kafka"""
        try:
            await self.kafka_manager.send_message(
                topic='analytics.api_events',
                message={
                    'method': method,
                    'path': path,
                    'status_code': status_code,
                    'duration_ms': duration_ms,
                    'user_id': user_id,
                    'timestamp': datetime.now().isoformat()
                }
            )
        except Exception as e:
            logger.error(f"Failed to stream event to Kafka: {e}")


async def stream_llm_inference(
    kafka_manager,
    model: str,
    prompt_length: int,
    response_length: int,
    latency_ms: float,
    source: str
):
    """Stream LLM inference metrics to Kafka"""
    try:
        await kafka_manager.send_message(
            topic='edge.ai.inference',
            message={
                'model': model,
                'prompt_length': prompt_length,
                'response_length': response_length,
                'latency_ms': latency_ms,
                'source': source,
                'timestamp': datetime.now().isoformat()
            },
            key=model
        )
    except Exception as e:
        logger.error(f"Failed to stream LLM metrics: {e}")
